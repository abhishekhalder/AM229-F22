{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Problem 1. [100 points] Hello Europa\n\nScientists speculate that beneath its thick icy surface, [Jupyter's moon Europa](https://en.wikipedia.org/wiki/Europa_(moon)) has huge salty water ocean that stays in liquid phase due to tidal effects. Some believe that Europa's subsurface ocean is a promising place to harbor extraterrestial life.\n\nConsider a robotic space mission that delivers $m$ rovers equipped with biochemical sensors on Europa's icy surface at **known locations** $a_{i}\\in\\mathbb{R}^{n}$ (for our application $n=2$ or $3$, but we will fix the numerical data later). All coordinates are measured w.r.t. a fixed reference frame (which is irrelevant for this problem). Upon landing, all rovers detect that a source located at **unknown location** $x\\in\\mathbb{R}^{n}$ is emitting biochemical signal. Specifically, the sensors onboard the rovers measure the noisy range $r_i$ between the source and the $i$th rover:\n$$r_i = \\|x-a_i\\|_2 + \\varepsilon_i, \\quad i=1,...,m,$$\nwhere $(\\varepsilon_1, ..., \\varepsilon_m)^{\\top}$ denotes the **unknown noise vector**. The purpose of this problem is to estimate the source location $x\\in\\mathbb{R}^{n}$ from the noisy measurements $r_i>0$.\n\n\n## (a) [15 + 5 = 20 points] Range Error Formulation\n\n(i) A natural way to formulate this problem is to solve\n$$\\underset{x\\in\\mathbb{R}^{n}}{\\min}\\sum_{i=1}^{m}\\left(r_i - \\|x-a_i\\|_2\\right)^{2},$$\nor equivalently,\n\\begin{align*}\n&\\underset{x\\in\\mathbb{R}^{n},g\\in\\mathbb{R}^{m}}{\\min}\\sum_{i=1}^{m}\\left(r_i - g_i\\right)^{2}\\\\\n&\\text{subject to} \\quad g_i^2 = \\|x-a_i\\|_2^{2} \\quad \\forall i=1,...,m.\n\\end{align*}\nUse the change-of-variables \n$$G := \\begin{pmatrix}g\\\\\n1\\end{pmatrix}\\left(g^{\\top} \\quad 1\\right), \\qquad X := \\begin{pmatrix}x\\\\\n1\\end{pmatrix}\\left(x^{\\top} \\quad 1\\right),$$\nto **re-write** the above as an optimization problem over the matrix decision variable pair $(X,G)$.\n\n(ii) **Mathematically argue** whether the optimization problem derived in part (a)(i) is convex or nonconvex. \n\n## Solution for Prob. 1(a)\n\n(i) Being outer products, the definition of the suggested change of variables $(x,g)\\in\\mathbb{R}^{n}\\times\\mathbb{R}^{m}\\mapsto(X,G)\\in\\mathbb{R}^{(n+1)\\times(n+1)}\\times\\mathbb{R}^{(m+1)\\times(m+1)}$ introduce the following contraints:\n$$X \\succeq 0, \\quad G \\succeq 0, \\quad \\text{rank}(X) = \\text{rank}(G) = 1, \\quad X_{n+1,n+1} = G_{m+1,m+1}=1.$$\nSo the optimization problem over the matrix decision variable pair $(X,G)$ becomes\n\\begin{align*}\n&\\underset{X\\in\\mathbb{S}_{+}^{n+1}, G\\in\\mathbb{S}_{+}^{m+1}}{\\text{minimize}}\\quad\\sum_{i=1}^{m}\\left(G_{ii}âˆ’2r_{i}G_{m+1,i}+r_i^2\\right)\\\\\n&\\qquad\\qquad\\qquad G_{i i}=\\operatorname{trace}\\left(F_i X\\right), \\quad i=1, \\ldots, m,\\\\\n&\\qquad\\qquad\\qquad G_{m+1, m+1}=X_{n+1, n+1}=1,\\\\\n&\\qquad\\qquad\\qquad \\operatorname{rank}(X)=\\operatorname{rank}(G)=1,\n\\end{align*}\nwhere $\nF_i:=\\left(\\begin{array}{cc}\nI_{n \\times n} & -a_i \\\\\n-a_i^{\\top} & \\left\\|a_i\\right\\|_2^2\n\\end{array}\\right), \\quad i=1, \\ldots, m$.\n\n(ii) The optimization problem derived in part (a)(i) is nonconvex **because the rank one constraints are nonconvex**, i.e., the set of matrices of rank one is a nonconvex set in any dimension. For example, in $2\\times 2$ case, the matrices $\\begin{pmatrix}\n1 & 0\\\\\n0 & 0\n\\end{pmatrix}$ and $\\begin{pmatrix}\n0 & 0\\\\\n0 & 1\n\\end{pmatrix}$ have both rank 1 but their convex combination $\\begin{pmatrix}\n\\theta & 0\\\\\n0 & 1-\\theta\n\\end{pmatrix}$ have rank 2 for any $0<\\theta < 1$. You can generalize this counterexample to any dimesnion by considering convex combination of matrices whose one column is a column of the identity matrix in that dimension and all other columns are zeros.\n\n## (b) [15 + 5 = 20 points] Squared Range Error Formulation\n\n(i) A different formulation is to solve\n$$\\underset{x\\in\\mathbb{R}^{n}}{\\min}\\underbrace{\\sum_{i=1}^{m}\\left(\\|x-a_i\\|_2^2 - r_i^2\\right)^{2}}_{f_0(x)},$$\nor equivalently,\n\\begin{align*}\n&\\underset{x\\in\\mathbb{R}^{n},\\alpha\\in\\mathbb{R}}{\\min}\\sum_{i=1}^{m}\\left(\\alpha - 2a_i^{\\top}x + \\|a_i\\|_2^2 - r_i^2\\right)^{2}\\\\\n&\\text{subject to} \\quad x^{\\top}x=\\alpha.\n\\end{align*}\nIntroducing $y:=\\begin{pmatrix}\nx\\\\\n\\alpha\n\\end{pmatrix}\\in\\mathbb{R}^{n+1}$, re-write the above problem as \n\\begin{align*}\n&\\underset{y\\in\\mathbb{R}^{n+1}}{\\min}\\|Ay-b\\|_2^{2}\\\\\n&\\text{subject to} \\quad y^{\\top}Cy + 2d^{\\top}y = 0.\n\\end{align*}\n**In other words, derive** $A,b,C,d$ as function of the problem data: $r_1^2,..., r_m^2$, and $a_1,..., a_m$.\n\n(ii) **Mathematically argue** whether the optimization problem derived in part (b)(i) is convex or nonconvex.\n\n## solution for Prob. 1(b)\n\n(i) Using the suggested change of variables, we directly obtain\n$$\nA=\\left(\\begin{array}{cc}\n-2 a_1^{\\top} & 1 \\\\\n\\vdots & \\vdots \\\\\n-2 a_m^{\\top} & 1\n\\end{array}\\right), \\quad b=\\left(\\begin{array}{c}\nr_1^2-\\left\\|a_1\\right\\|_2^2 \\\\\n\\vdots \\\\\nr_m^2-\\left\\|a_m\\right\\|_2^2\n\\end{array}\\right), \\quad C=\\left(\\begin{array}{cc}\nI_{n \\times n} & 0_{n \\times 1} \\\\\n0_{1 \\times 0} & 0\n\\end{array}\\right), \\quad d=\\left(\\begin{array}{c}\n0_{n \\times 1} \\\\\n-0.5\n\\end{array}\\right).\n$$\n\n(ii) The optimization problem derived in part (b)(i) is a **nonconvex** QCQP. The nonconvexity is due to the quadratic **equality** constraint, which is a nonconvex set.\n\n## (c) [25 + 35 = 60 points] Numerical Solution\n\n(i) Let us call the problem derived in part (b)(i) as the primal problem. It can be theoretically proved that this primal problem has zero duality gap. This theoretical knowledge suggests the following strategy: derive the Lagrange dual problem for the primal in part (b)(i), then numerically solve that Lagrange dual problem via cvx/cvxpy/Convex.jl, then invoke strong duality. \n\n**Using this strategy, compute the optimal estimate of the source location** $x^{\\rm{opt}}\\in\\mathbb{R}^{2}$ for $m=5$ rovers located at\n$$a_1=\\begin{pmatrix}\n1.8\\\\\n2.5\n\\end{pmatrix},\\quad a_2=\\begin{pmatrix}\n2.0\\\\\n1.7\n\\end{pmatrix},\\quad a_3=\\begin{pmatrix}\n1.5\\\\\n1.5\n\\end{pmatrix},\\quad a_4=\\begin{pmatrix}\n1.5\\\\\n2.0\n\\end{pmatrix},\\quad a_5=\\begin{pmatrix}\n2.5\\\\\n1.5\n\\end{pmatrix},$$\nand $r=(2.00, 1.24, 0.59, 1.31, 1.44)$. **Please submit your code**.\n\nFor the above data, see figure file $\\texttt{f0Contour.png}$ in CANVAS file section, folder: Homework Problems and Solutions, showing the contour lines of the objective $f_0(x)$ in part (b) with rover locations $a_i$ denoted as circles.\n\n(ii) Let us now execute an alternative strategy to solve the primal problem in part (b) as follows. Use the KKT condition to write the primal argmin $y^{\\rm{opt}}$ as an explicit function of the optimal Lagrange multiplier $\\nu^{\\rm{opt}}$. Then derive a nonlinear algebraic equation of the form $\\phi(\\nu^{\\rm{opt}})=0$ and solve the same using bisection method for the numerical data given in part (c)(i). Finally, **compute and compare** the $x^{\\rm{opt}}$ obtained from part c(ii) and c(i).\n\n## Solution for Prob. 1(c)\n\n(i) Denote the Lagrange multiplier associated with the single equality constraint as $\\nu\\in\\mathbb{R}$. Then the Lagrangian $L(y, \\nu)=\\|A y-b\\|_2^2+\\nu\\left(y^{\\top} C y+2 d^{\\top} y\\right)$ yields the Lagrange dual function\n$$\ng(\\nu)= \\begin{cases}-\\left(A^{\\top} b-\\nu d\\right)^{\\top}\\left(A^{\\top} A+\\nu C\\right)^{\\dagger}\\left(A^{\\top} b-\\nu d\\right)+b^{\\top} b, & \\text { if } A^{\\top} A+\\nu C \\succeq 0 \\\\ -\\infty, & \\text { otherwise. }\\end{cases}\n$$\nTherefore, the Lagrange dual problem is\n$$\n\\begin{aligned}\n&\\underset{\\nu \\in \\mathbb{R}}{\\operatorname{minimize}}\\left(A^{\\top} b-\\nu d\\right)^{\\top}\\left(A^{\\top} A+\\nu C\\right)^{\\dagger}\\left(A^{\\top} b-\\nu d\\right)-b^{\\top} b \\\\\n&\\text { subject to } \\quad A^{\\top} A+\\nu C \\succeq 0\n\\end{aligned}\n$$\nA cvxpy code to solve the above is given below.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport numpy.linalg as la\nimport cvxpy as cvx\nimport cvxopt\n\nn = 2 # dimension\nm = 5 # number of sensors\n# problem data\na = np.array([[1.8, 2.5], \n              [2.0, 1.7], \n              [1.5, 1.5], \n              [1.5,2.0], \n              [2.5,1.5]])\nr = np.array([2.00, 1.24, 0.59, 1.31, 1.44])\n# parameters in part (b) formulation as function of the problem data\nA = np.hstack([-2*a, np.ones((m,1))])\nb = r**2 - (la.norm(a,axis=1)**2)\nC = np.eye(n+1); C[-1,-1] = 0\nd = np.zeros(n+1); d[-1] = -0.5\n\n# the dual problem\nnu = cvx.Variable()\n\nobjective = cvx.Maximize( - cvx.atoms.matrix_frac(np.matmul(A.T,b) - nu*d, \n                                       np.matmul(A.T,A) + nu*C) + np.matmul(b.T,b))\n\nconstraints = [np.matmul(A.T, A) + nu*C >> 0]\n\n# solve using CVXOPT\nprob = cvx.Problem(objective, constraints)\nresult = prob.solve(solver=cvx.CVXOPT)\n\nyopt = np.matmul(la.inv(np.matmul(A.T, A) + nu.value*C), np.matmul(A.T, b) - nu.value*d)\n\nprint('Optimal source location is xopt = ', yopt[:-1])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "(ii) Thanks to strong duality, the optimal primal-dual variable pair $(y^{\\rm{opt}},\\nu^{\\rm{opt}})$ for part (b) satisfies the KKT condition:\n$$\n\\begin{aligned}\n&\\text { (gradient condition) }\\left(A^{\\top} A+\\nu^{\\mathrm{opt}} C\\right) y^{\\mathrm{opt}}=A^{\\top} b-\\nu^{\\mathrm{opt}} d,\\\\\n&\\text { (primal feasibility) }\\left(y^{\\mathrm{opt}}\\right)^{\\top} C y^{\\mathrm{opt}}+2 d^{\\top} y^{\\mathrm{opt}}=0, \\\\\n&\\text { (feasibility of the dual problem) } A^{\\top} A+\\nu^{\\mathrm{opt}} C \\succeq 0.\n\\end{aligned}\n$$\nFrom the gradient condition, we can write $y^{\\rm{opt}}$ (hence $x^{\\rm{opt}}$) in terms of $\\nu^{\\rm{opt}}$ as\n$$y^{\\text {opt }}\\left(\\nu^{\\text {opt }}\\right)=\\left(A^{\\top} A+\\nu^{\\text {opt }} C\\right)^{\\dagger}\\left(A^{\\top} b-\\nu^{\\text {opt }} d\\right).$$\nSubstituting the above functional relation in the primal feasibility gives a scalar nonlinear equation $\\phi(\\nu^{\\rm{opt}}) = 0$, where $\\nu^{\\rm{opt}}\\in\\mathcal{I}$, an interval comprising of all $\\nu^{\\rm{opt}}$ satisfying feasibility of the dual problem, i.e., $A^{\\top} A+\\nu^{\\mathrm{opt}} C \\succeq 0$. Specifically,\n$$\n\\phi\\left(\\nu^{\\text {opt }}\\right):=\\left(A^{\\top} b-\\nu^{\\text {opt }} d\\right)^{\\top}\\left(A^{\\top} A+\\nu^{\\text {opt }} C\\right)^{\\dagger} C\\left(A^{\\top} A+\\nu^{\\text {opt }} C\\right)^{\\dagger}\\left(A^{\\top} b-\\nu^{\\text {opt }} d\\right)+2 d^{\\top}\\left(A^{\\top} A+\\nu^{\\text {opt }} C\\right)^{\\dagger}\\left(A^{\\top} b-\\nu^{\\text {opt }} d\\right)=0.\n$$\nNotice that if $A^{\\top} A+\\nu^{\\mathrm{opt}} C \\succ 0$ (which is indeed the case for our numerical data), then we can replace the pseudo-inverse by inverse, and in that case, the interval $\\mathcal{I} = \\left(-1/\\lambda_{\\max}\\left(\\left(A^{\\top}A\\right)^{-1/2}C\\left(A^{\\top}A\\right)^{-1/2}\\right),+\\infty\\right)$. In fact, you may (but you were not required to) verify by direct derivative computation that $\\phi(\\cdot)$ is decreasing over $\\mathcal{I}$, i.e., if a real root exists in $\\mathcal{I}$ then it must be unique. \n\nWe perform bisection to compute $\\nu^{\\text {opt }}$, and thus $x^{\\text {opt }}$ as shown in the sample code below. We obtain $\\nu^{\\rm{opt}}$ from KKT condition as $0.589609183371067$ while the same from dual solution via CVXOPT is $0.58995428439989606$. From KKT condition, we obtain $x^{\\text {opt }}$ as $(1.32688595,0.64457943)$ which matches well with the same from dual solution via CVXOPT: $(1.32697669,0.64472181)$.  ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def phi(nuopt):\n    \n    yopt = np.matmul(la.inv(np.matmul(A.T, A) + nuopt*C),np.matmul(A.T,b) - nuopt*d)\n    \n    return np.matmul(np.matmul(yopt.T,C),yopt) + 2*np.matmul(d.T,yopt)\n\ndef bisection(func, tol, left, right):\n    current_val  = float('inf')\n    current = 0\n    while abs(current_val) >= tol:\n        mid = (left + right)/2\n        left_val = func(left)\n        mid_val = func(mid)\n        right_val = func(right)\n        current_val = mid_val\n        current = mid\n        if left_val > 0 and right_val < 0:\n            if mid_val > 0:\n                left = mid\n            else:\n                right = mid\n        elif left_val < 0 and right_val > 0:\n            if mid_val > 0:\n                right = mid\n            else:\n                left = mid\n        else:\n            return float('-inf')\n    return current\n\nnuoptKKT = bisection(phi, 1e-8, 0.0, 10.0)\nprint(\"From KKT condition, nuopt = \", nuoptKKT)\nprint('From dual solution via CVXOPT, nuopt = ', nu.value)\n\nyoptKKT = np.matmul(la.inv(np.matmul(A.T, A) + nuoptKKT*C),np.matmul(A.T,b) - nuoptKKT*d)\nprint('From KKT condition, xopt = ', yoptKKT[:-1])\nprint('From dual solution via CVXOPT, xopt = ', yopt[:-1])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}